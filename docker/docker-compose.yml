# =======================================================================
# Platform Name            demo-platform
# Platform Stack:          trivadis/platys-modern-data-platform
# Platform Stack Version:  1.15.0-preview
# =======================================================================
version: '3.5'
networks:
  default:
    name: demo-platform
# backward compatiblity to platform < 1.14.0
# Enable PostgreSQL or MySQL for MLflow server
services:
  #  ================================== Apache Spark 2.x ========================================== #
  spark-master:
    image: trivadis/apache-spark-master:3.1.3-hadoop3.2
    container_name: spark-master
    hostname: spark-master
    labels:
      com.platys.name: spark
      com.platys.webui.title: Spark UI
      com.platys.webui.url: http://${PUBLIC_IP}:8080
    ports:
      - 6066:6066
      - 7077:7077
      - 8080:8080
      - 4040-4044:4040-4044
    env_file:
      - ./conf/hadoop.env
    environment:
      CORE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      CORE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      HIVE_SITE_CONF_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:?PLATYS_AWS_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:?PLATYS_AWS_SECRET_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: s3.amazonaws.com
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'False'
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      INIT_DAEMON_STEP: setup_spark
      MASTER: spark://spark-master:7077
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages:
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  spark-worker-1:
    image: trivadis/apache-spark-worker:3.1.3-hadoop3.2
    container_name: spark-worker-1
    hostname: spark-worker-1
    labels:
      com.platys.name: spark
    depends_on:
      - spark-master
    ports:
      - 28111:28111
    env_file:
      - ./conf/hadoop.env
    environment:
      SPARK_MASTER: spark://spark-master:7077
      SPARK_WORKER_WEBUI_PORT: '28111'
      SPARK_WORKER_OPTS: -Dspark.worker.cleanup.enabled=true
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      CORE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      CORE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      HIVE_SITE_CONF_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:?PLATYS_AWS_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:?PLATYS_AWS_SECRET_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: s3.amazonaws.com
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'False'
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages:
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  spark-worker-2:
    image: trivadis/apache-spark-worker:3.1.3-hadoop3.2
    container_name: spark-worker-2
    hostname: spark-worker-2
    labels:
      com.platys.name: spark
    depends_on:
      - spark-master
    ports:
      - 28112:28112
    env_file:
      - ./conf/hadoop.env
    environment:
      SPARK_MASTER: spark://spark-master:7077
      SPARK_WORKER_WEBUI_PORT: '28112'
      SPARK_WORKER_OPTS: -Dspark.worker.cleanup.enabled=true
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      CORE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      CORE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      HIVE_SITE_CONF_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:?PLATYS_AWS_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:?PLATYS_AWS_SECRET_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: s3.amazonaws.com
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'False'
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages:
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  #  ================================== NiFi ========================================== #
  nifi-1:
    image: apache/nifi:1.16.3
    container_name: nifi-1
    hostname: nifi-1
    labels:
      com.platys.name: nifi
      com.platys.webui.title: Apache NiFi UI Node 18080
      com.platys.webui.url: https://${PUBLIC_IP}:18080/nifi
    ports:
      # HTTP
      - 18080:18080
      # Remote Input Socket
      - 10005:10005/tcp
    environment:
      NIFI_WEB_HTTPS_PORT: '18080'
      NIFI_WEB_HTTPS_HOST: nifi-1
      NIFI_WEB_PROXY_HOST: ${PUBLIC_IP}:18080,${DOCKER_HOST_IP}18080
      AUTH: tls
      KEYSTORE_PATH: /opt/certs/keystore.jks
      KEYSTORE_TYPE: JKS
      KEYSTORE_PASSWORD: PTz7kFl1rzX4wUtcDlurwV6gjm7vID9Ibgbe71N355w
      TRUSTSTORE_PATH: /opt/certs/truststore.jks
      TRUSTSTORE_TYPE: JKS
      TRUSTSTORE_PASSWORD: h8I20cJyr50rFwzJRZkBcodLM8ifnDqQK2kORds8TLk
      NIFI_SECURITY_USER_AUTHORIZER: single-user-authorizer
      NIFI_SECURITY_USER_LOGIN_IDENTITY_PROVIDER: single-user-provider
      NIFI_REMOTE_INPUT_SOCKET_PORT: '10005'
      NIFI_REMOTE_INPUT_HOST: nifi-1
      NIFI_SENSITIVE_PROPS_KEY: 12345678901234567890A
      SINGLE_USER_CREDENTIALS_USERNAME: nifi
      SINGLE_USER_CREDENTIALS_PASSWORD: 1234567890ACD
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/nifi/keystore.jks:/opt/certs/keystore.jks
      - ./conf/nifi/truststore.jks:/opt/certs/truststore.jks
    restart: unless-stopped
  #  ================================== Zeppelin ========================================== #
  zeppelin:
    image: trivadis/apache-zeppelin:0.10.0-spark3.1.3-hadoop3.2
    container_name: zeppelin
    hostname: zeppelin
    labels:
      com.platys.name: zeppelin
      com.platys.webui.title: Apache Zeppelin UI
      com.platys.webui.url: http://${PUBLIC_IP}:28080
    ports:
      - 28080:8080
      - 6060:6060
      - 5050:5050
      - 4050-4054:4050-4054
    env_file:
      - ./conf/hadoop.env
    environment:
      CORE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      CORE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_endpoint: s3.amazonaws.com
      HIVE_SITE_CONF_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:?PLATYS_AWS_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:?PLATYS_AWS_SECRET_ACCESS_KEY must be set either in .env or as an environment variable}
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'False'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: s3.amazonaws.com
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'False'
      SPARK_HADOOP_FS_S3A_ACCESS_KEY: ${PLATYS_AWS_ACCESS_KEY:?PLATYS_AWS_ACCESS_KEY must be set either in .env or as an environment variable}
      SPARK_HADOOP_FS_S3A_SECRET_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:?PLATYS_AWS_SECRET_ACCESS_KEY must be set either in .env or as an environment variable}
      # for awscli & s3cmd
      AWS_ACCESS_KEY_ID: ${PLATYS_AWS_ACCESS_KEY:?PLATYS_AWS_ACCESS_KEY must be set either in .env or as an environment variable}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      AWS_ENDPOINT: s3.amazonaws.com
      AWS_DEFAULT_REGION: eu-central-1
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages:
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
      ZEPPELIN_ADDR: 0.0.0.0
      ZEPPELIN_PORT: '8080'
      ZEPPELIN_MEM: -Xms1024m -Xmx1024m -XX:MaxMetaspaceSize=512m
      ZEPPELIN_INTERPRETER_CONNECT_TIMEOUT: 120000
      ZEPPELIN_INTERPRETER_DEP_MVNREPO: https://repo.maven.apache.org/maven2
      ZEPPELIN_ADMIN_USERNAME: admin
      ZEPPELIN_ADMIN_PASSWORD: changeme
      ZEPPELIN_USER_USERNAME: zeppelin
      ZEPPELIN_USER_PASSWORD: changeme
      # set spark-master for Zeppelin interpreter
      ZEPPELIN_SPARK_MASTER: spark://spark-master:7077
      ZEPPELIN_NOTEBOOK_DIR: notebook
      ZEPPELIN_NOTEBOOK_CRON_ENABLE: 'True'
      PYSPARK_PYTHON: python3
      SPARK_SUBMIT_OPTIONS: ' --conf spark.ui.port=4050 --conf spark.driver.host=zeppelin --conf spark.driver.port=5050 --conf spark.driver.bindAddress=0.0.0.0 --conf spark.blockManager.port=6060 --conf spark.driver.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4 --conf spark.executor.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4'
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
      - ./conf/s3cfg:/root/.s3cfg.template
    restart: unless-stopped
  #  ================================== Elasticsearch ========================================== #
  elasticsearch-1:
    image: elasticsearch:7.17.0
    hostname: elasticsearch-1
    container_name: elasticsearch-1
    labels:
      com.platys.name: elasticsearch
      com.platys.restapi.title: Elasticsearch REST API
      com.platys.restapi.url: http://${PUBLIC_IP}:9200
      com.platys.manual.step.msgs: sudo sysctl -w vm.max_map_count=262144
    ports:
      - 9200:9200
      - 9300:9300
    environment:
      discovery.type: single-node
      xpack.security.enabled: 'false'
      xpack.monitoring.enabled: 'false'
      http.cors.enabled: 'true'
      http.cors.allow-origin: http://${DOCKER_HOST_IP}:28275,http://${PUBLIC_IP}:28275,http://dejavu:1358,http://dataplatform:28125,http://dataplatform:28125,http://${PUBLIC_IP}:28125,http://${DOCKER_HOST_IP}:28125,http://127.0.0.1:1358
      http.cors.allow-headers: X-Requested-With,X-Auth-Token,Content-Type,Content-Length,Authorization
      http.cors.allow-credentials: 'true'
      ES_JAVA_OPTS: -Xms512m -Xmx512m
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== ElasticVue ========================================== #
  elasticvue:
    image: cars10/elasticvue:latest
    container_name: elasticvue
    hostname: elasticvue
    labels:
      com.platys.name: elasticvue
      com.platys.webui.title: ElasticVue UI
      com.platys.webui.url: http://${PUBLIC_IP}:28275
    ports:
      - 28275:8080
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== OpenSearch ========================================== #
  opensearch-1:
    image: opensearchproject/opensearch:2
    hostname: opensearch-1
    container_name: opensearch-1
    labels:
      com.platys.name: opensearch
      com.platys.restapi.title: OpenSearch REST API
      com.platys.restapi.url: http://${PUBLIC_IP}:29200
      com.platys.manual.step.msgs: sudo sysctl -w vm.max_map_count=262144
    ports:
      - 29200:9200
      - 29600:9600
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-1
      - discovery.type=single-node
      - DISABLE_INSTALL_DEMO_CONFIG=true     # disables execution of install_demo_configuration.sh bundled with security plugin, which installs demo certificates and security configurations to OpenSearch
      - DISABLE_SECURITY_PLUGIN=true     #  disables security plugin entirely in OpenSearch by setting plugins.security.disabled: true in opensearch.yml
      - bootstrap.memory_lock=true   # along with the memlock settings below, disables swapping
      - OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m   # minimum and maximum Java heap size, recommend setting both to 50% of system RAM
      - http.cors.enabled=true
      - http.cors.allow-origin="http://${DOCKER_HOST_IP}:28275,http://${PUBLIC_IP}:28275,http://dejavu:1358,http://dataplatform:28125,http://dataplatform:28125,http://${PUBLIC_IP}:28125,http://${DOCKER_HOST_IP}:28125,http://127.0.0.1:1358"
      - http.cors.allow-headers="X-Requested-With,X-Auth-Token,Content-Type,Content-Length,Authorization"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536 # maximum number of open files for the OpenSearch user, set to at least 65536 on modern systems
        hard: 65536
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== OpenSearch Dashboards ========================================== #
  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2
    hostname: opensearch-dashboards
    container_name: opensearch-dashboards
    labels:
      com.platys.name: opensearch-dashboards
      com.platys.webui.title: OpenSearch Dashboards
      com.platys.webui.url: http://${PUBLIC_IP}:5603
    ports:
      - 5603:5601
    expose:
      - '5601'
    environment:
      - OPENSEARCH_HOSTS=["http://opensearch-1:9200"]
      - DISABLE_SECURITY_DASHBOARDS_PLUGIN=true     # disables security dashboards plugin in OpenSearch Dashboards
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== PostgreSQL ========================================== #
  postgresql:
    image: postgres:13
    container_name: postgresql
    hostname: postgresql
    labels:
      com.platys.name: postgresql
    ports:
      - 5432:5432
    environment:
      - POSTGRES_PASSWORD=abc123!
      - POSTGRES_USER=postgres
      - POSTGRES_DB=postgres
      - POSTGRES_MULTIPLE_DATABASES=demodb
      - POSTGRES_MULTIPLE_USERS=demo
      - POSTGRES_MULTIPLE_PASSWORDS=abc123!
      - PGDATA=/var/lib/postgresql/data/pgdata
      - DB_SCHEMA=demo
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/postgresql:/docker-entrypoint-initdb.d/
    restart: unless-stopped
  #  ================================== NocoDB ========================================== #
  nocodb:
    image: nocodb/nocodb:latest
    container_name: nocodb
    hostname: nocodb
    labels:
      com.platys.name: nocodb
      com.platys.webui.title: NocoDB UI
      com.platys.webui.url: http://${PUBLIC_IP}:28276
    ports:
      - 28276:8080
    environment:
      NC_DB: pg://postgresql:5432?u=postgres&p=abc123!&d=postgres
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Wetty ========================================== #
  wetty:
    image: svenihoney/wetty:latest
    container_name: wetty
    hostname: wetty
    labels:
      com.platys.name: wetty
      com.platys.webui.title: WeTTY UI
      com.platys.webui.url: http://${PUBLIC_IP}:3001
    ports:
      - 3001:3000
    environment:
      - REMOTE_SSH_SERVER=${DOCKER_HOST_IP}
      - REMOTE_SSH_PORT=22
      - REMOTE_SSH_USER=
      - WETTY_PORT=3000
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== markdown-viewer ========================================== #
  markdown-viewer:
    image: trivadis/markdown-web:latest
    container_name: markdown-viewer
    hostname: markdown-viewer
    labels:
      com.platys.name: markdown-viewer
      com.platys.webui.title: Markdown Viewer UI
      com.platys.webui.url: http://${PUBLIC_IP}:80
    ports:
      - 80:80
    volumes:
      - ./artefacts:/home/python/markdown
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  markdown-renderer:
    image: trivadis/jinja2-renderer:latest
    container_name: markdown-renderer
    hostname: markdown-renderer
    labels:
      com.platys.name: markdown-renderer
    environment:
      USE_PUBLIC_IP: 'True'
      PUBLIC_IP: ${PUBLIC_IP}
      DOCKER_HOST_IP: ${DOCKER_HOST_IP}
      DATAPLATFORM_HOME: ${DATAPLATFORM_HOME}
      PLATYS_PLATFORM_NAME: demo-platform
      PLATYS_PLATFORM_STACK: trivadis/platys-modern-data-platform
      PLATYS_PLATFORM_STACK_VERSION: 1.15.0-preview
      PLATYS_COPY_COOKBOOK_DATA: 'True'
    volumes:
      - ./artefacts/templates:/templates
      - ./artefacts/templates:/scripts
      - .:/variables
      - ./artefacts:/output
      - ./data-transfer:/data-transfer
volumes:
  data-transfer-vol:
    name: data_transfer_vol
